# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ybmf2VSjg6K_qU8dldQFJPcICD01plWk
"""

# installing catboost
!pip install catboost

from google.colab import drive
drive.mount('/content/drive')

"""# Importing Libraries"""

import numpy as np
import pandas as pd;
import matplotlib.pyplot as plt;
import seaborn as sns;
from sklearn.impute import SimpleImputer;#to handle missing data
from sklearn.compose import ColumnTransformer;#to apply specific transformations to each type of data
from sklearn.pipeline import Pipeline;
from sklearn.preprocessing import LabelEncoder;
from sklearn.preprocessing import StandardScaler;
from sklearn.preprocessing import MinMaxScaler; #scale features to a specified range, typically between 0 and 1
from sklearn.model_selection import train_test_split;
from sklearn.linear_model import LinearRegression ;
from sklearn.linear_model import LogisticRegression;
from sklearn.linear_model import Ridge, Lasso;
from sklearn.metrics import mean_squared_error;
from catboost import CatBoostRegressor
from sklearn.svm import SVR;
from sklearn.svm import SVC;
from fastai.tabular.all import add_datepart;
from sklearn.tree import DecisionTreeClassifier;
from sklearn.tree import DecisionTreeRegressor;
from sklearn.ensemble import RandomForestClassifier;
from sklearn.ensemble import RandomForestRegressor;
from sklearn.neighbors import KNeighborsClassifier;
from sklearn.neighbors import KNeighborsRegressor;
import xgboost as xgb;
from xgboost import XGBClassifier;
from xgboost import XGBRegressor;
import re #for manipulating strings with regular expressions
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import mean_squared_error, confusion_matrix
import pickle

train_df = pd.read_csv('/content/drive/MyDrive/ML_project/Train.csv')
test_df= pd.read_csv('/content/drive/MyDrive/ML_project/Test.csv');
sample_df = pd.read_csv('/content/drive/MyDrive/ML_project/SampleSubmission.csv')

"""# Understanding the data"""

#Checking the columns in the dataset.
print('The columns in the train dataset are :',train_df.columns)
print('\n')
print('The columns in the test dataset are :',test_df.columns)

#shape for the datasets
train_df.shape, test_df.shape, sample_df.shape

"""Train dataset"""

# Previewing the top of our dataset
train_df.head()

# checking train info
train_df.info()

# Checking whether each column has an appropriate datatype
train_df.dtypes

# we check for null values
train_df.isnull().sum()

"""The train dataset has no null/missing values"""

# checking the unique values in each column
for column in train_df.columns:
  print("***************************", column, "********************************")
  print("This column has", train_df[column].nunique(), "unique values, they are shown below:\n")
  print(train_df[column].unique())
  print('________________________________________________________________________________________________\n')

"""Test dataset"""

# Previewing the top of our dataset
test_df.head()

# checking the data set info
test_df.info()

# check the unique values in each column
for column in test_df.columns:
  print("***************************", column, "********************************")
  print("This column has", test_df[column].nunique(), "unique values, they are shown below:\n")
  print(test_df[column].unique())
  print('________________________________________________________________________________________________\n')

# Checking whether each column has an appropriate datatype
test_df.dtypes

"""# Cleaning the data

## Train data
"""

# converting the incorrect datatypes to right data types
train_df['DATOP'] = pd.to_datetime(train_df['DATOP'])
train_df['FLTID'] = train_df['FLTID'].astype('object')
train_df['DEPSTN'] = train_df['DEPSTN'].astype('object')
train_df['ARRSTN'] = train_df['ARRSTN'].astype('object')
train_df['STD'] = pd.to_datetime(train_df['STD'])

# replacing period with colons to get the time column to the right format
train_df['STA'] = train_df['STA'].str.replace('.', ':')
train_df['STA'] = pd.to_datetime(train_df['STA'])
train_df['STATUS'] = train_df['STATUS'].astype('object')
train_df['AC'] = train_df['AC'].astype('object')

# Checking for anomalies in the date columns
# first we check split the dates into year, month, day , hr, and min for granular insights
# splitting DATOP into year, month, day
train_df['year_datop'] = pd.to_datetime(train_df['DATOP']).dt.year
train_df['month_datop'] = pd.to_datetime(train_df['DATOP']).dt.month
train_df['day_datop'] = pd.to_datetime(train_df['DATOP']).dt.day

# splitting STD into hours and minutes
train_df['std_hr'] = pd.to_datetime(train_df['STD']).dt.hour
train_df['std_min'] = pd.to_datetime(train_df['STD']).dt.minute

# splitting STA into hours and minutes
train_df['sta_hr'] = pd.to_datetime(train_df['STA']).dt.hour
train_df['sta_min'] = pd.to_datetime(train_df['STA']).dt.minute

# confirming that the split happened
train_df.head()

# checking the years in the data set
train_df['year_datop'].unique()

"""We are covering Flight data between 2016 and 2018"""

# checking the months in the data set
train_df['month_datop'].unique()

"""flight were recorded throughout the yeras"""

# checking the days in the data set
train_df['day_datop'].unique()

# checking the hours in the data set
train_df['std_hr'].unique()

# check the minutes in the data set
train_df['std_min'].unique()

# check the scheduled time of arrival hours in the data set
train_df['sta_hr'].unique()

# check the scheduled time of arrival minutes in the data set
train_df['sta_min'].unique()

# More data cleaning procedures
# changing the names of columns so that they make more sense

train_df.rename(columns={'ID': 'id', 'DATOP': 'date_flight', 'FLTID': 'flight_number', 'DEPSTN': 'departure_point', 'ARRSTN': 'arrival_point', 'STD': 'scheduled_time_departure',
             'STA': 'scheduled_time_arrival', 'STATUS': 'status', 'AC': 'aircraft_code', 'target': 'delayed_minutes'}, inplace=True)

# confirming that the column names have been changed
train_df.head()

"""## Test dataset"""

# converting the incorrect datatypes to right data types
test_df['DATOP'] = pd.to_datetime(test_df['DATOP'])
test_df['FLTID'] = test_df['FLTID'].astype('object')
test_df['DEPSTN'] = test_df['DEPSTN'].astype('object')
test_df['ARRSTN'] = test_df['ARRSTN'].astype('object')
test_df['STD'] = pd.to_datetime(test_df['STD'])

# replacing period with column to get the time column in to the right format
test_df['STA'] = test_df['STA'].str.replace('.', ':')
test_df['STA'] = pd.to_datetime(test_df['STA'])
test_df['STATUS'] = test_df['STATUS'].astype('object')
test_df['AC'] = test_df['AC'].astype('object')

# confirming that the columns have been converted to the right types
test_df.dtypes

# Checking for anomalies in the date columns
# first we check split the dates into year, month, day , hr, and min
# splitting DATOP into year, month, day
test_df['year_datop'] = pd.to_datetime(test_df['DATOP']).dt.year
test_df['month_datop'] = pd.to_datetime(test_df['DATOP']).dt.month
test_df['day_datop'] = pd.to_datetime(test_df['DATOP']).dt.day

# splitting STD into hours and minutes
test_df['std_hr'] = pd.to_datetime(test_df['STD']).dt.hour
test_df['std_min'] = pd.to_datetime(test_df['STD']).dt.minute

# splitting STA into hours and minutes
test_df['sta_hr'] = pd.to_datetime(test_df['STA']).dt.hour
test_df['sta_min'] = pd.to_datetime(test_df['STA']).dt.minute

# confirming that the split happened
test_df.head()

# checking the years in the data set
test_df['year_datop'].unique()

# checking the months in the data set
test_df['month_datop'].unique()

# checking the days in the data set
test_df['day_datop'].unique()

# checking the scheduled departure time hour in the data set
test_df['std_hr'].unique()

# checking the scheduled departure time minute in the data set
test_df['std_min'].unique()

# checking the scheduled arrival time hour in the data set
test_df['sta_hr'].unique()

# checking the scheduled arrival time minute in the data set
test_df['sta_min'].unique()

# More data cleaning procedures
# changing the names of columns so that they make more sense

test_df.rename(columns={'ID': 'id','DATOP': 'date_flight', 'FLTID': 'flight_number', 'DEPSTN': 'departure_point', 'ARRSTN': 'arrival_point', 'STD': 'scheduled_time_departure',
             'STA': 'scheduled_time_arrival', 'STATUS': 'status', 'AC': 'aircraft_code', 'target': 'delayed_minutes'}, inplace=True)

# confirming that the column names have been changed
test_df.head()

"""# **Feature Engineering**"""

# using the datepart on the date_flight column to get the date summaries
test_df = add_datepart(test_df,'date_flight')
train_df= add_datepart(train_df,'date_flight')

# preview the train data
train_df.head()

# preview the test data
test_df.head()

# Creating a function to be used to create delay or not
def result(delayed_minutes):
  if delayed_minutes > 0:
    return 'delayed'
  elif delayed_minutes == 0:
    return 'on time'
  else:
    return 'early'

# Applying the result function to the dataframe
train_df['result'] = train_df.delayed_minutes.apply(lambda x: result(x))
#train_df.drop(columns = 'date_flightElapsed', inplace = True)
# Previewing the changes
train_df.head(2)

# Creating a seasons column
data = [train_df, test_df]
for dataset in data:
    dataset.loc[ (dataset['month_datop'] < 3) | (dataset['month_datop'] == 12), 'Season'] = "Winter"
    dataset.loc[(dataset['month_datop'] >= 3) & (dataset['month_datop'] < 6), 'Season'] = "spring"
    dataset.loc[(dataset['month_datop'] >= 6) & (dataset['month_datop'] < 9), 'Season'] = "summer"
    dataset.loc[(dataset['month_datop'] >= 9) & (dataset['month_datop'] < 12), 'Season'] = "fall"

# check the seasons column
print(train_df['Season'].unique())
print(test_df['Season'].unique())

# find the week of the month
data = [train_df, test_df]
for dataset in data:
    dataset.loc[ dataset['day_datop'] <= 7, 'WeekofMonth'] = "Week 1"
    dataset.loc[(dataset['day_datop'] > 7) & (dataset['day_datop'] <= 14), 'WeekofMonth'] = "Week 2"
    dataset.loc[(dataset['day_datop'] > 14) & (dataset['day_datop'] <= 21), 'WeekofMonth'] = "Week 3"
    dataset.loc[(dataset['day_datop'] > 21) & (dataset['day_datop'] <= 28), 'WeekofMonth'] = "Week 4"
    dataset.loc[(dataset['day_datop'] > 28) & (dataset['day_datop'] <= 31), 'WeekofMonth'] = "Week 5"

# get the unique values
print(train_df['WeekofMonth'].unique())
print(test_df['WeekofMonth'].unique())

train_df.head()

# convert to date time and the required format
# scheduled time departure column
train_df['scheduled_time_departure'] = pd.to_datetime(train_df['scheduled_time_departure'], format='%Y-%m-%d %H:%M:%S')
test_df['scheduled_time_departure'] = pd.to_datetime(test_df['scheduled_time_departure'], format='%Y-%m-%d %H:%M:%S')

# scheduled time arrival column
train_df['scheduled_time_arrival'] = pd.to_datetime(train_df['scheduled_time_arrival'], format='%Y-%m-%d %H:%M:%S')

# Display the first few
test_df['scheduled_time_arrival'] = pd.to_datetime(test_df['scheduled_time_arrival'], format='%Y-%m-%d %H.%M.%S')

"""# EDA

## Univariete Analysis
"""

# determine the measures of dispersion of the price column
print(train_df["delayed_minutes"].var())
print(train_df["delayed_minutes"].std())
print(train_df["delayed_minutes"].skew())
print(train_df["delayed_minutes"].kurt())

train_df.hist(bins=40, figsize=(20, 12))
plt.show()

"""

*   Delayed time has a moderate variance and is positively skewed

"""

plt.figure(figsize=(6, 4))
sns.countplot(x='WeekofMonth', data=train_df, palette="GnBu_d", order=train_df['WeekofMonth'].value_counts().index)
plt.title('Flights per Week')
plt.xlabel('Week of Month')
plt.ylabel('Count')
plt.show()

"""

*  Week 4 has a slightly higher number of flights as compared to the other weeks with slightly higher than 25000 flights weekly as compared to the other weeks which have their value counts slightly lower than 25000.

"""

plt.figure(figsize=(6, 4))
sns.countplot(x='result', data=train_df, palette="GnBu_d")
plt.show()

"""

*   There are more delayed flights as compared to the flights that were on time for the period given(2016-2018) overally.

"""

plt.figure(figsize=(6, 4))
sns.countplot(x='status', data=train_df, palette="GnBu_d")
plt.show()

"""

*   There was a higher number of fights that arrived at the Actual Time of Arrival(ATA).

"""

plt.figure(figsize=(6, 4))
sns.countplot(x='Season', data=train_df, palette="GnBu_d", order=train_df['Season'].value_counts().index)
plt.title('Flight count by season')
plt.xlabel('Season')
plt.ylabel('Count')
plt.show()

"""

*   The hottest season / summer is in June, July, August and September. Tunis has dry periods in June, July and August. The warmest month is August with an average maximum temperature of 34Â°C (94Â°F).

*   This in turn makes it a high/peak season as most people would prefer to travel for holidays(summer)

"""

plt.figure(figsize=(6, 4))
sns.countplot(x='year_datop', data=train_df, palette="GnBu_d", order=train_df['year_datop'].value_counts().index)
plt.title('Flight count by year')
plt.xlabel('Season')
plt.ylabel('Count')
plt.show()

train_df['year_datop'].value_counts()

plt.title('Flight delay count by year')
sns.countplot(x='year_datop',data=train_df, hue= train_df['result'])
plt.show()

plt.title('Flight delay count by month')
sns.countplot(x='month_datop',data=train_df, hue= train_df.result)
plt.show()

plt.title('Flight delay count season')
sns.countplot(x='WeekofMonth',data=train_df, hue= train_df.result)
plt.show()

"""## Multivariet Analysis"""

sns.pairplot(train_df, hue="result", corner = True)
plt.show()

"""# Model Training

## CatBoost Regressor
"""

#dropping the id column
train_df1=train_df.drop(['id'],axis=1)
test_df1=test_df.drop(['id'],axis=1)

train_df1=train_df.drop(['result'],axis=1)

#assigning variables
y=train_df1['delayed_minutes']
X=train_df1.drop(['delayed_minutes'],axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)

def preprocess_data(df):
    for col in df.columns:
        if np.issubdtype(df[col].dtype, np.datetime64):
            df[col] = df[col].astype(str)
        elif df[col].dtype == bool:
            df[col] = df[col].astype(int)
    return df

X_train = preprocess_data(X_train)
X_test = preprocess_data(X_test)
test_df1 = preprocess_data(test_df)

categ_id = np.where(X_train.dtypes == 'object')[0]
print(f'Categorical feature indices: {categ_id}')

CatB = CatBoostRegressor(
    iterations=700, depth=5, l2_leaf_reg=7, learning_rate=0.03,
    one_hot_max_size=2, task_type='CPU')

CatB.fit(X_train, y_train, verbose=False, cat_features=categ_id)

# Scoring the model
model_score = CatB.score(X_test, y_test)
print(f'Model score: {model_score}')

importances = pd.DataFrame({'feature':X.columns,'importance':np.round(CatB.feature_importances_,3)})
importances = importances.sort_values('importance',ascending=False).set_index('feature')

importances.plot.bar()
plt.title("Important features in determining whether a flight is delayed ")
plt.ylabel('Feature importances')
plt.xlabel('Features')

predcat = CatB.predict(X_test)

predcat=CatB.predict(X_test)

test2=pd.read_csv('/content/drive/MyDrive/ML_project/Test.csv')

mse = np.round(mean_squared_error(y_test, predcat),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

from sklearn.metrics import mean_absolute_error

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, predcat)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# results = {'ID': test2['ID'], 'target':pred}
# results = pd.DataFrame(results)
# results.to_csv('CatBoost_results.csv', index=False)

"""Hyperparametr Tuning"""

from sklearn.model_selection import RandomizedSearchCV

# catboost_model = CatBoostRegressor(task_type='CPU', cat_features=categ_id, verbose=0, early_stopping_rounds=10)
# param_dist = {
#     'iterations': [100, 300, 500, 700],
#     'depth': [4, 5, 6, 7, 8],
#     'learning_rate': [0.01, 0.03, 0.1, 0.2],
#     'l2_leaf_reg': [1, 3, 5, 7, 9]
# }

# # Perform RandomizedSearchCV
# random_search = RandomizedSearchCV(estimator=catboost_model, param_distributions=param_dist, n_iter=10, cv=3, scoring='neg_mean_squared_error', random_state=42)
# random_search.fit(X_train, y_train)

# best_params = random_search.best_params_
# best_score = random_search.best_score_

# print(f'Best parameters: {best_params}')
# print(f'Best score: {best_score}')

# # Train the final model with best parameters
# final_model = CatBoostRegressor(**best_params, task_type='CPU', verbose=False)
# final_model.fit(X_train, y_train, cat_features=categ_id)

# # Making predictions on test_df
# predcat_ = final_model.predict(test_df1)

# results = {'ID': test2['ID'], 'target':pred}
# results = pd.DataFrame(results)
# results.to_csv('CatBoost_improved_results.csv', index=False)

"""*   Since the tunned model doesn't vary much from the original model, then we try out other models

## Challenging the solution

### Further feature Engineering
"""

train_df.head()

test_df.head()

train_df['std_month']=train_df['scheduled_time_departure'].dt.month
train_df['sta_month']=train_df['scheduled_time_arrival'].dt.month
train_df['std_day']=train_df['scheduled_time_departure'].dt.day
train_df['sta_day']=train_df['scheduled_time_arrival'].dt.day

test_df['scheduled_time_departure']=pd.to_datetime(test_df['scheduled_time_departure'])
test_df['scheduled_time_arrival']=pd.to_datetime(test_df['scheduled_time_arrival'])

test_df['std_month']=test_df['scheduled_time_departure'].dt.month
test_df['sta_month']=test_df['scheduled_time_arrival'].dt.month
test_df['std_day']=test_df['scheduled_time_departure'].dt.day
test_df['sta_day']=test_df['scheduled_time_arrival'].dt.day

train_df=train_df.drop(['scheduled_time_arrival','scheduled_time_departure'], axis=1)
test_df=test_df.drop(['scheduled_time_arrival','scheduled_time_departure'], axis=1)

train_df.head()

le=LabelEncoder()

train_df['id'] = le.fit_transform(train_df['id']);
train_df['flight_number'] = le.fit_transform(train_df['flight_number']);
train_df['departure_point'] = le.fit_transform(train_df['departure_point'])
train_df['arrival_point'] = le.fit_transform(train_df['arrival_point']);
train_df['status'] = le.fit_transform(train_df['status']);
train_df['aircraft_code'] = le.fit_transform(train_df['aircraft_code'])
train_df['Season'] = le.fit_transform(train_df['Season'])
train_df['WeekofMonth'] = le.fit_transform(train_df['WeekofMonth'])

train_df['date_flightIs_month_end'] = le.fit_transform(train_df['date_flightIs_month_end']);
train_df['date_flightIs_month_start'] = le.fit_transform(train_df['date_flightIs_month_start']);
train_df['date_flightIs_quarter_end'] = le.fit_transform(train_df['date_flightIs_quarter_end'])
train_df['date_flightIs_quarter_start'] = le.fit_transform(train_df['date_flightIs_quarter_start']);
train_df['date_flightIs_year_end'] = le.fit_transform(train_df['date_flightIs_year_end'])

test_df['id'] = le.fit_transform(test_df['id']);
test_df['flight_number'] = le.fit_transform(test_df['flight_number']);
test_df['departure_point'] = le.fit_transform(test_df['departure_point'])
test_df['arrival_point'] = le.fit_transform(test_df['arrival_point']);
test_df['status'] = le.fit_transform(test_df['status']);
test_df['aircraft_code'] = le.fit_transform(test_df['aircraft_code'])
test_df['Season'] = le.fit_transform(test_df['Season'])
test_df['WeekofMonth'] = le.fit_transform(test_df['WeekofMonth'])

test_df['date_flightIs_month_end'] = le.fit_transform(test_df['date_flightIs_month_end']);
test_df['date_flightIs_month_start'] = le.fit_transform(test_df['date_flightIs_month_start']);
test_df['date_flightIs_quarter_end'] = le.fit_transform(test_df['date_flightIs_quarter_end'])
test_df['date_flightIs_quarter_start'] = le.fit_transform(test_df['date_flightIs_quarter_start']);
test_df['date_flightIs_year_end'] = le.fit_transform(test_df['date_flightIs_year_end'])

train_df.head()

train_df=train_df.drop(['result'], axis=1)

train_df.select_dtypes(include=['bool']).columns

train_df['date_flightIs_year_start'] = le.fit_transform(train_df['date_flightIs_year_start'])
test_df['date_flightIs_year_start'] = le.fit_transform(test_df['date_flightIs_year_start'])

y=train_df['delayed_minutes']
X=train_df.drop(['delayed_minutes'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)

"""### XGBoost Regresor"""

xgb_model = XGBRegressor(objective='reg:squarederror', n_estimators=100, max_depth=5, learning_rate=0.1)

# Train the model
xgb_model.fit(X_train, y_train)

xgb_score=xgb_model.score(X_test, y_test)
xgb_score

# Make predictions
pred = xgb_model.predict(X_test)

mse = np.round(mean_squared_error(y_test, pred),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# results = {'ID': test2['ID'], 'target':pred}
# results = pd.DataFrame(results)
# results.to_csv('XGBoost_results.csv', index=False)

"""### Ridge"""

rid = Ridge();
rid.fit(X_train, y_train);
ridge_score=rid.score(X_test, y_test)
ridge_score

predrid  = rid.predict(X_test)

# results = {'ID': test2['ID'], 'target':predrid}
# results = pd.DataFrame(results)
# results.to_csv('ridge_results.csv', index=False)

mse=np.round(mean_squared_error(y_test, predrid),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, predrid)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

"""### Random Forest"""

random_f = RandomForestRegressor();
random_f.fit(X_train, y_train);
random_f_score=random_f.score(X_test, y_test)
random_f_score

predrandom_f  = random_f.predict(X_test)

mse = np.round(mean_squared_error(y_test, predrandom_f),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, predrandom_f)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# results = {'ID': test2['ID'], 'target':predrandom_f}
# results = pd.DataFrame(results)
# results.to_csv('randomF_results.csv', index=False)

"""### Decision Tree Regressor"""

dc = DecisionTreeRegressor();
dc.fit(X_train, y_train);
dc_score=dc.score(X_test, y_test)
dc_score

preddc=dc.predict(X_test)

mse = np.round(mean_squared_error(y_test, preddc),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, preddc)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# results = {'ID': test2['ID'], 'target':preddc}
# results = pd.DataFrame(results)
# results.to_csv('DecisonT.csv', index=False)

"""### KNNeighbor Regressor"""

knn = KNeighborsRegressor();
knn.fit(X_train, y_train);
knn_score=knn.score(X_test, y_test)
knn_score

predknn=knn.predict(X_test)

mse = np.round(mean_squared_error(y_test, predknn),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

# Plot actual vs. predicted values
plt.figure(figsize=(8, 6))
plt.scatter(y_test, predknn)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.title('Actual vs. Predicted Values')
plt.show()

# results = {'ID': test2['ID'], 'target':predknn}
# results = pd.DataFrame(results)
# results.to_csv('knn.csv', index=False)

"""### NN"""

# Define the neural network architecture
seq = Sequential()
seq.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
seq.add(Dropout(0.2))
seq.add(Dense(32, activation='relu'))
seq.add(Dropout(0.2))
seq.add(Dense(1))  # Regression output

# Compile the model
seq.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
history=seq.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

# seq_score=seq.score(X_test, y_test)
# seq_score

# Make predictions
predseq = history.predict(X_test)

# # Evaluate the model
mse = np.round(mean_squared_error(y_test, predseq),2)
rmse = np.round(np.sqrt(mse),2)
mse, rmse

flattened_predseq = predseq.flatten()

# results = {'ID': test2['ID'], 'target':flattened_predseq}
# results = pd.DataFrame(results)
# results.to_csv('NN_results.csv', index=False)

mse, rmse

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation accuracy values
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""### COWRF"""

# Define the Cuckoo Search parameters
n_nests = 25
n_iterations = 100
pa = 0.25  # Discovery rate of alien eggs/solutions
beta = 1.5  # Parameter for Levy flight

# Define the objective function
def objective_function(params):
    n_estimators, max_depth = int(params[0]), int(params[1])
    model = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return mean_squared_error(y_test, y_pred)

# Initialize nests
nests = np.zeros((n_nests, 2))
nests[:, 0] = np.random.randint(50, 300, size=n_nests)  # n_estimators
nests[:, 1] = np.random.randint(5, 20, size=n_nests)  # max_depth

best_nest = nests[0, :]
best_score = objective_function(best_nest)

import scipy
from scipy.special import gamma

# Levy flight function
def levy_flight(Lambda):
    sigma = (gamma(1 + Lambda) * np.sin(np.pi * Lambda / 2) /
             (gamma((1 + Lambda) / 2) * Lambda * 2**((Lambda - 1) / 2)))**(1 / Lambda)
    u = np.random.randn() * sigma
    v = np.random.randn()
    step = u / abs(v)**(1 / Lambda)
    return step

# Cuckoo Search optimization loop
for iteration in range(n_iterations):
    for i in range(n_nests):
        new_nest = nests[i, :] + levy_flight(beta)
        new_nest[0] = int(np.clip(new_nest[0], 50, 300))  # Clipping the range for n_estimators
        new_nest[1] = int(np.clip(new_nest[1], 5, 20))  # Clipping the range for max_depth

        new_score = objective_function(new_nest)
        if new_score < best_score:
            best_nest = new_nest
            best_score = new_score
        nests[i, :] = new_nest if new_score < objective_function(nests[i, :]) else nests[i, :]

# Discover new solutions
    for i in range(n_nests):
        if np.random.rand() < pa:
            nests[i, :] = np.random.randint(50, 300), np.random.randint(5, 20)

    print(f'Iteration {iteration + 1}/{n_iterations}, Best Score: {best_score}')

# Train the best model found
best_model = RandomForestRegressor(n_estimators=int(best_nest[0]), max_depth=int(best_nest[1]), random_state=42)
best_model.fit(X_train, y_train)
predCOWRF = best_model.predict(test_df)
# final_rmse = mean_squared_error(y_test, y_pred, squared=False)
# print(f'Final RMSE: {final_rmse}')

results = {'ID': test2['ID'], 'target':predCOWRF}
results = pd.DataFrame(results)
results.to_csv('COWRF_results.csv', index=False)

"""# Deploying Model"""

# Saving model to disk
pickle.dump(CatB, open('/content/drive/MyDrive/ML_project/Model_deployment/CatB.pkl','wb'))